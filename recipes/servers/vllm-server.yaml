name: vllm-server
description: vLLM single-node inference server for MeluXina

service:
  command: "module load env/release/2024.1 && module load Apptainer/1.3.6-GCCcore-13.3.0 && CACHE_DIR=/project/scratch/$SLURM_JOB_ACCOUNT/$USER/vllm && CONTAINER_DIR=/project/scratch/$SLURM_JOB_ACCOUNT/$USER/containers && IMAGE=$CONTAINER_DIR/vllm-openai_v0.5.4.sif && mkdir -p $CACHE_DIR && mkdir -p $CONTAINER_DIR && if [ ! -f $IMAGE ]; then echo 'Pulling vLLM container image - may take 5 to 10 minutes' && apptainer pull $IMAGE docker://vllm/vllm-openai:v0.5.4; else echo 'Using existing container image'; fi && HOSTNAME=$(hostname) && PORT=8000 && echo '' && echo '=========================================' && echo 'vLLM Server Configuration' && echo '=========================================' && echo 'Endpoint: http://'$HOSTNAME':'$PORT && echo 'Model: facebook/opt-125m' && echo 'Device: CPU' && echo '=========================================' && echo '' && echo 'IMPORTANT: Update your client recipe with:' && echo 'endpoint: http://'$HOSTNAME':'$PORT && echo '' && echo '=========================================' && echo '' && apptainer exec --bind $CACHE_DIR:/root/.cache/huggingface $IMAGE bash -c 'set -e && echo Starting vLLM server on CPU && python3 -m vllm.entrypoints.openai.api_server --model facebook/opt-125m --host 0.0.0.0 --port 8000 --device cpu --dtype auto --max-model-len 2048 --disable-log-requests'"

  working_dir: .

  env:
    VLLM_HOST: "0.0.0.0"
    VLLM_PORT: "8000"

  ports:
    - 8000

orchestration:
  resources:
    cpu_cores: 8
    memory_gb: 128
    partition: cpu
    time_limit: "02:00:00"
